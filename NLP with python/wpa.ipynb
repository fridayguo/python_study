{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "\n",
    "#import nltk\n",
    "#from nltk.book import *\n",
    "import jieba\n",
    "\n",
    "#-*-coding:utf-8-*-\n",
    "\n",
    "user_dict = './NLP with python/wpa_dict.txt'\n",
    "stop_dict = './NLP with python/stop_word_dict.txt'\n",
    "replace_file = './NLP with python/replace_file.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除标点符号\n",
    "def trim_puntuation(msg):\n",
    "    import re \n",
    "    tmp_str = msg.decode(\"utf8\")\n",
    "    #print tmp_str\n",
    "    puntuation = \"[0-9\\s+\\.\\!\\/_,$%^*(+\\\"\\'\\\\]+|[+——！，。？?、~@#￥%……&*（）]+\".decode(\"utf8\")\n",
    "    #print puntuation   \n",
    "    res_str = re.sub(puntuation, \" \".decode(\"utf8\"), tmp_str)\n",
    "    return res_str\n",
    "\n",
    "#过滤停用词  \n",
    "import codecs\n",
    "def trim_stop_word(word_list, stop_word_file):\n",
    "    st = codecs.open(stop_word_file, 'rb',encoding='utf-8')      \n",
    "    stopwords = [ line.strip() for line in st ]  \n",
    "    return [ w for w in word_list if len(w.strip()) >0 and w not in stopwords ]\n",
    "\n",
    "def fen_ci(msg):\n",
    "    # print msg \n",
    "    # msg = str(msg).decode('utf8')\n",
    "    text_pun = trim_puntuation(msg)\n",
    "    word_list = trim_stop_word(jieba.lcut(text_pun), stop_dict)\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5232 entries, 0 to 7489\nData columns (total 14 columns):\nstat_day           5232 non-null int64\nkfuin              5232 non-null int64\nqq                 5232 non-null int64\nis_b2c             5232 non-null int64\nis_delay           5232 non-null int64\nis_not_ack         5232 non-null int64\nmsg_c2b            5232 non-null int64\nmsg_b2c            5232 non-null int64\nrecv_msg_num       5232 non-null int64\nsend_msg_num       5232 non-null int64\nfirst_recv_date    5232 non-null object\nfirst_send_date    3883 non-null object\nis_spam            5232 non-null int64\nmsg                5232 non-null object\ndtypes: int64(11), object(3)\nmemory usage: 613.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./NLP with python/wpa_msg_20171123.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "#df.info()\n",
    "\n",
    "# 只考虑C2B会话\n",
    "c2b_df = df[df.is_b2c == 0]\n",
    "c2b_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(user_dict)\n",
    "df['words'] = map(fen_ci, map(str, df['msg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               下   在 \n1                                            多少一个企业QQ \n2     1   企业号可以找回吗   你好   e   11   这个   企业员工号被盗了怎么找...\n3                                             营销QQ怎么卖 \n4                                              zaima  \n5                           我的QQ开了至尊保但被封了   QQ能换手机号码吗 \n6                            在吗？   QQ空间被举报只有自己能进去看的了。 \n7     我QQ  为啥加不上人？   我给100   300   500   600   1000...\n8             在？   企业号可以开2000人群？   是的   把你的这么回复关了   0 \n9                                       我想免费要一个5位号的qq \nName: msg, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['msg'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    下\n1                                             一个 企业 QQ\n2    企业 号 找回 你好 e 企业 员工 号 被盗 找回 手机 密码 改了 手机 绑定 还 找回...\n3                                              营销 QQ 卖\n4                                                zaima\n5                              QQ 开 至尊 保但 封 QQ 能换 手机号码\n6                                        QQ 空间 举报 进去 看\n7                                     QQ 为啥 加不上 人 ko \\\n8                                       企业 号 开 人群 回复 关\n9                                        想 免费 一个 位号 qq\nName: words, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['words'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQ -> 77  企业 -> 66  你好 -> 39  不 -> 36  好 -> 31  qq -> 30  申请 -> 29  没 -> 27  营销 -> 23  下 -> 22  想 -> 21  说 -> 19  网站 -> 15  一个 -> 15  都 -> 14  公司 -> 13  微信 -> 12  加 -> 12  谢谢 -> 12  价格 -> 11 \n"
     ]
    }
   ],
   "source": [
    "# 统计所有的字段的频率，给出top10\n",
    "def word_freq_top(word_list, top_k=10):\n",
    "    from collections import Counter\n",
    "    wc = Counter(word_list)\n",
    "    sort_list = sorted(wc.items(), key = lambda d:d[1], reverse=True)\n",
    "    for w,c in sort_list[: top_k]:\n",
    "        print \"%s -> %d \" %(w,c),\n",
    "        \n",
    "word_list = [w for words in df['words'][:100] for w in words.split()]\n",
    "word_freq_top(word_list,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple -> 3  egg -> 2  banana -> 1  企业 -> 7  QQ -> 7  手机 -> 4  工号 -> 4  一个 -> 4  号 -> 4  找回 -> 3  换 -> 3  开 -> 2  人 -> 2 \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "word_freq_top(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (343, 15) test_df.shape: (4889, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 只有那些20171101-20171106这六天，那些客服没有回复的会话，都标注过\n",
    "\n",
    "c2b_df['test_flag'] = 1\n",
    "c2b_df.loc[(c2b_df.is_not_ack == 1) & (c2b_df.stat_day >= 20171101) \n",
    "           & (c2b_df.stat_day<=20171106) & (c2b_df.is_spam != 2),\n",
    "           'test_flag' ] = 0 \n",
    "train_df = c2b_df[c2b_df.test_flag == 0]\n",
    "test_df = c2b_df[c2b_df.test_flag == 1]\n",
    "print 'train_df.shape:', train_df.shape, 'test_df.shape:', test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    261\n0     82\nName: is_spam, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['is_spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 下   在   没定单啊   不是   是申请的   网上申请的企业   qq   嗯嗯   多少钱呢   暂时2   个   多少一个企业QQ   你好   企业qq换手机在哪换   网址发一下   谢谢   ?   企业QQ换手机   怎么换   500人的企业号   多少钱一个   嗯   一个工号120?   那100个工号呢   有10个工号的吗   我人不是很多   只需10个工号   价格是多少   这个怎么购买呢   个人现在还不行吗   官网能发下吗   我找不到  nan  zaima   nan nan  我QQ  为啥加不上人？   我给100   300   500   600   1000   500000   56.636   6200000   ko?\\   在？   企业号可以开2000人群？   是的   把你的这么回复关了   0  nan  你好   企业QQ30个多少钱一年   标准的   贵了   你这价格   有人买5000   扯蛋   百度我都查了   你应该是代理   你是代理还是   哦哦   代理也买6000   坑啊   能用一年就好了   不要了   你们自己用吧   那为什么人家买的比你\n"
     ]
    }
   ],
   "source": [
    "text = all_text[:1001]\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n        0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n        1, 0, 0, 0, 1, 1, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "corpus = df['words'][:10]\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 63)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko/nan/qq/zaima/一个/不到/不行/为啥/人群/价格/企业/你好/加不上/发下/哪换/回复/多少钱/官网/定单/工号/很多/我人/手机/暂时/申请/网上/网址/谢谢/购买\n"
     ]
    }
   ],
   "source": [
    "print '/'.join(vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}